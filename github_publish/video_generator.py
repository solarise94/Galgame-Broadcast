#!/usr/bin/env python3
"""
è§†é¢‘æ’­å®¢ç”Ÿæˆå™¨ - å°†éŸ³é¢‘è½¬æ¢ä¸ºå¸¦å­—å¹•çš„è§†é¢‘æ’­å®¢
æ”¯æŒå­—å¹•ã€è¯´è¯äººæ ‡ç­¾ã€æ¸å˜èƒŒæ™¯ç­‰åŠŸèƒ½

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ä½¿ç”¨æ–¹æ³•:                                                        â•‘
â•‘    python video_generator.py [é€‰é¡¹]                               â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  ä¾èµ–å®‰è£…:                                                        â•‘
â•‘    pip install moviepy Pillow numpy                              â•‘
â•‘    # macOS éœ€è¦ ffmpeg: brew install ffmpeg                      â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  å¸¸ç”¨å‘½ä»¤:                                                        â•‘
â•‘    # åŸºç¡€ç”¨æ³• (ä½¿ç”¨æ¸å˜èƒŒæ™¯)                                      â•‘
â•‘    python video_generator.py -i audio_output -o podcast.mp4      â•‘
â•‘                                                                   â•‘
â•‘    # æŒ‡å®š Markdown æ–‡ä»¶ (ç”¨äºæå–å­—å¹•æ–‡æœ¬)                        â•‘
â•‘    python video_generator.py -i audio_output -m æ–‡æ¡ˆ.md -o output.mp4
â•‘                                                                   â•‘
â•‘    # ä½¿ç”¨çº¯è‰²èƒŒæ™¯                                                 â•‘
â•‘    python video_generator.py -i audio -o out.mp4 -b color        â•‘
â•‘                                                                   â•‘
â•‘    # ä½¿ç”¨è‡ªå®šä¹‰èƒŒæ™¯å›¾ç‰‡                                           â•‘
â•‘    python video_generator.py -i audio -o out.mp4 -b image --bg-path bg.jpg
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  èƒŒæ™¯ç±»å‹è¯´æ˜:                                                    â•‘
â•‘    â€¢ gradient (é»˜è®¤) - è“/ç´«æ¸å˜è‰²ï¼Œç”·å£°è“è‰²ã€å¥³å£°ç´«è‰²           â•‘
â•‘    â€¢ color           - çº¯è‰²èƒŒæ™¯                                   â•‘
â•‘    â€¢ image           - è‡ªå®šä¹‰å›¾ç‰‡èƒŒæ™¯                             â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import os
import re
import yaml
import time
import argparse
import tempfile
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass

# ä¼˜å…ˆä½¿ç”¨ç³»ç»Ÿ FFmpegï¼ˆHomebrew å®‰è£…çš„ FFmpeg 8.0+ï¼‰
if os.path.exists('/opt/homebrew/bin/ffmpeg'):
    os.environ['IMAGEIO_FFMPEG_EXE'] = '/opt/homebrew/bin/ffmpeg'
    print(f"ğŸ¬ ä½¿ç”¨ç³»ç»Ÿ FFmpeg: /opt/homebrew/bin/ffmpeg")

import numpy as np
from PIL import Image, ImageDraw, ImageFont
try:
    # moviepy 2.x
    from moviepy import (
        AudioFileClip, ImageClip, CompositeVideoClip, TextClip,
        concatenate_videoclips, ColorClip, concatenate_audioclips
    )
    from moviepy.video.fx import FadeIn, FadeOut
except ImportError:
    # moviepy 1.x
    from moviepy.editor import (
        AudioFileClip, ImageClip, CompositeVideoClip, TextClip,
        concatenate_videoclips, ColorClip, concatenate_audioclips
    )
    from moviepy.video.fx.all import fadein, fadeout


@dataclass
class DialogueSegment:
    """å¯¹è¯æ®µè½æ•°æ®"""
    index: int
    speaker: str  # 'male' æˆ– 'female'
    text: str
    audio_path: str
    duration: float
    mood: str = "gentle"  # æƒ…ç»ª: gentle, happy, confident, expectant, confused, shocked, angry, sad, resigned


class SubtitleGenerator:
    """å­—å¹•ç”Ÿæˆå™¨"""
    
    # æ”¯æŒçš„æƒ…ç»ªåˆ—è¡¨
    MOODS = ['gentle', 'happy', 'confident', 'expectant', 'confused', 
             'shocked', 'angry', 'sad', 'resigned']
    
    def __init__(self, font_path: str = None, font_size: int = 40, style: str = "default", 
                 enable_mood: bool = True, avatar_base_path: str = "avatar",
                 galgame_avatar_config: Dict = None):
        self.font_size = font_size
        self.font_path = font_path or self._get_default_font()
        self.style = style  # "default" æˆ– "galgame"
        self.avatar_size = 100  # å¤´åƒå°ºå¯¸
        self.enable_mood = enable_mood  # æ˜¯å¦å¯ç”¨æƒ…ç»ªç«‹ç»˜
        self.avatar_base_path = avatar_base_path  # ç«‹ç»˜åŸºç¡€è·¯å¾„
        self.galgame_avatar_config = galgame_avatar_config or {}  # GalGame ç«‹ç»˜é…ç½®
        self.avatars = self._load_avatars()
        self.min_lines = 2  # æœ€å°‘æ˜¾ç¤º2è¡Œï¼ˆä¿è¯å­—å¹•æ¡†æœ‰ä¸€å®šé«˜åº¦ï¼‰
        self.max_lines = 6  # æœ€å¤šæ˜¾ç¤º6è¡Œï¼ˆé˜²æ­¢è¿‡é•¿ï¼‰
        self.line_height = 52  # æ¯è¡Œé«˜åº¦
        self.bg_padding = 40  # å­—å¹•æ¡†ä¸Šä¸‹è¾¹è·
        
        # å­—å¹•é•¿åº¦é™åˆ¶é…ç½®
        self.max_chars_per_screen = 80  # æ¯å±æœ€å¤šå­—ç¬¦æ•°ï¼ˆæ›´ä¸¥æ ¼çš„é™åˆ¶ï¼‰
        self.max_subtitle_parts = 4  # æœ€å¤šæ‹†åˆ†å‡ æ®µ
        self.min_chars_for_split = 60  # è¶…è¿‡è¿™ä¸ªé•¿åº¦æ‰è€ƒè™‘æ‹†åˆ†
    
    def split_long_text(self, text: str, video_width: int = 1920) -> List[Tuple[str, float]]:
        """
        å°†é•¿æ–‡æœ¬æ‹†åˆ†æˆå¤šä¸ªé€‚åˆå±å¹•æ˜¾ç¤ºçš„å­å­—å¹•ï¼Œå¹¶è®¡ç®—æ¯æ®µçš„ç›¸å¯¹æ—¶é•¿
        
        ç­–ç•¥ï¼š
        1. å¦‚æœæ–‡æœ¬è¾ƒçŸ­ï¼Œç›´æ¥è¿”å›
        2. ä¼˜å…ˆæŒ‰å¥å­æ‹†åˆ†ï¼ˆã€‚ï¼ï¼Ÿï¼›ï¼‰
        3. å¦‚æœå¥å­ä»å¤ªé•¿ï¼ŒæŒ‰é€—å·æˆ–é•¿åº¦æ‹†åˆ†
        4. æ ¹æ®å­—ç¬¦æ•°å’Œæ ‡ç‚¹ç¬¦å·è®¡ç®—æ¯æ®µçš„ç›¸å¯¹æ—¶é•¿
        
        Returns:
            [(å­å­—å¹•æ–‡æœ¬, ç›¸å¯¹æ—¶é•¿æ¯”ä¾‹), ...]
            ç›¸å¯¹æ—¶é•¿æ¯”ä¾‹æ€»å’Œä¸º 1.0
        """
        # ä½¿ç”¨é…ç½®çš„æœ€å¤§å­—ç¬¦æ•°é™åˆ¶
        max_chars_total = self.max_chars_per_screen
        
        # å¦‚æœæ–‡æœ¬ä¸é•¿ï¼Œä¸éœ€è¦æ‹†åˆ†
        if len(text) <= self.min_chars_for_split:
            return [(text, 1.0)]
        
        parts = []
        remaining = text
        
        # å¥å­ç»“æŸç¬¦
        sentence_ends = 'ã€‚ï¼ï¼Ÿï¼›'
        
        while remaining and len(parts) < self.max_subtitle_parts:
            # å°è¯•æ‰¾åˆ°åˆé€‚çš„æ‹†åˆ†ç‚¹
            cut_pos = self._find_cut_position(remaining, max_chars_total, sentence_ends)
            
            if cut_pos > 0:
                parts.append(remaining[:cut_pos].strip())
                remaining = remaining[cut_pos:].strip()
            else:
                # æ‰¾ä¸åˆ°å¥½çš„æ‹†åˆ†ç‚¹ï¼Œå¼ºåˆ¶æŒ‰é•¿åº¦æ‹†åˆ†
                cut_pos = min(max_chars_total, len(remaining))
                parts.append(remaining[:cut_pos])
                remaining = remaining[cut_pos:]
        
        # å¦‚æœè¿˜æœ‰å‰©ä½™ï¼Œåˆå¹¶åˆ°æœ€åä¸€æ®µ
        if remaining:
            if len(parts) < self.max_subtitle_parts:
                parts.append(remaining)
            else:
                parts[-1] = parts[-1] + remaining
        
        if not parts:
            return [(text, 1.0)]
        
        # è®¡ç®—æ¯æ®µçš„ç›¸å¯¹æ—¶é•¿ï¼ˆåŸºäºå­—ç¬¦æ•° + æ ‡ç‚¹åœé¡¿ï¼‰
        part_weights = []
        for part in parts:
            weight = self._calculate_part_weight(part)
            part_weights.append(weight)
        
        # å½’ä¸€åŒ–å¾—åˆ°æ—¶é—´æ¯”ä¾‹
        total_weight = sum(part_weights)
        result = []
        for part, weight in zip(parts, part_weights):
            ratio = weight / total_weight if total_weight > 0 else 1.0 / len(parts)
            result.append((part, ratio))
        
        return result
    
    def _calculate_part_weight(self, text: str) -> float:
        """
        è®¡ç®—æ–‡æœ¬æ®µçš„æœ—è¯»æƒé‡
        
        åŸºäºï¼š
        - åŸºç¡€å­—ç¬¦æ•°ï¼ˆæ¯ä¸ªå­—ç¬¦ç®— 1ï¼‰
        - æ ‡ç‚¹ç¬¦å·å¢åŠ åœé¡¿æ—¶é—´
        """
        # åŸºç¡€æƒé‡ = å­—ç¬¦æ•°
        weight = len(text)
        
        # å¥å­ç»“æŸç¬¦å¢åŠ åœé¡¿ï¼ˆç›¸å½“äº 0.5 ä¸ªå­—ç¬¦æ—¶é—´ï¼‰
        weight += text.count('ã€‚') * 0.5
        weight += text.count('ï¼') * 0.5
        weight += text.count('ï¼Ÿ') * 0.5
        weight += text.count('ï¼›') * 0.5
        
        # é€—å·å¢åŠ çŸ­åœé¡¿ï¼ˆç›¸å½“äº 0.3 ä¸ªå­—ç¬¦æ—¶é—´ï¼‰
        weight += text.count('ï¼Œ') * 0.3
        weight += text.count('ã€') * 0.2
        
        return max(weight, 1.0)  # è‡³å°‘ 1.0 çš„æƒé‡
    
    def _find_cut_position(self, text: str, max_len: int, sentence_ends: str) -> int:
        """æ‰¾åˆ°æœ€ä½³æ‹†åˆ†ä½ç½®"""
        # é™åˆ¶æœç´¢èŒƒå›´
        search_end = min(len(text), max_len)
        
        # 1. ä¼˜å…ˆæ‰¾å¥å­ç»“æŸç¬¦
        for i in range(search_end - 1, -1, -1):
            if text[i] in sentence_ends:
                return i + 1
        
        # 2. å…¶æ¬¡æ‰¾é€—å·
        for i in range(search_end - 1, -1, -1):
            if text[i] in 'ï¼Œ,':
                return i + 1
        
        # 3. æ‰¾ç©ºæ ¼ï¼ˆè‹±æ–‡ï¼‰
        for i in range(search_end - 1, -1, -1):
            if text[i] == ' ':
                return i + 1
        
        return 0  # æ‰¾ä¸åˆ°å¥½çš„æ‹†åˆ†ç‚¹
        
    def _load_avatars(self) -> Dict[str, Image.Image]:
        """åŠ è½½å¤´åƒå›¾ç‰‡ï¼ˆæ”¯æŒæƒ…ç»ªç«‹ç»˜ï¼‰"""
        avatars = {}
        
        if self.enable_mood:
            # å¯ç”¨æƒ…ç»ªåŠŸèƒ½ï¼šåŠ è½½æ‰€æœ‰æƒ…ç»ªç«‹ç»˜
            for speaker in ['male', 'female']:
                for mood in self.MOODS:
                    key = f"{speaker}_{mood}"
                    path = f"{self.avatar_base_path}/{speaker}-{mood}.png"
                    if os.path.exists(path):
                        try:
                            img = Image.open(path).convert('RGBA')
                            # è°ƒæ•´å¤§å°ä¸ºåœ†å½¢å¤´åƒ
                            img = img.resize((self.avatar_size, self.avatar_size), Image.Resampling.LANCZOS)
                            # åˆ›å»ºåœ†å½¢é®ç½©
                            mask = Image.new('L', (self.avatar_size, self.avatar_size), 0)
                            mask_draw = ImageDraw.Draw(mask)
                            mask_draw.ellipse((0, 0, self.avatar_size, self.avatar_size), fill=255)
                            # åº”ç”¨åœ†å½¢é®ç½©
                            img.putalpha(mask)
                            avatars[key] = img
                        except Exception as e:
                            print(f"âš  åŠ è½½å¤´åƒå¤±è´¥ {path}: {e}")
            
            if avatars:
                print(f"âœ“ å·²åŠ è½½ {len(avatars)} ä¸ªæƒ…ç»ªç«‹ç»˜")
        
        # åŒæ—¶åŠ è½½é»˜è®¤ç«‹ç»˜ä½œä¸ºåå¤‡
        default_paths = {
            'male': f'{self.avatar_base_path}/male.png',
            'female': f'{self.avatar_base_path}/female.png'
        }
        
        for speaker, path in default_paths.items():
            if os.path.exists(path):
                try:
                    img = Image.open(path).convert('RGBA')
                    img = img.resize((self.avatar_size, self.avatar_size), Image.Resampling.LANCZOS)
                    mask = Image.new('L', (self.avatar_size, self.avatar_size), 0)
                    mask_draw = ImageDraw.Draw(mask)
                    mask_draw.ellipse((0, 0, self.avatar_size, self.avatar_size), fill=255)
                    img.putalpha(mask)
                    avatars[speaker] = img
                    print(f"âœ“ åŠ è½½é»˜è®¤å¤´åƒ: {path}")
                except Exception as e:
                    print(f"âš  åŠ è½½é»˜è®¤å¤´åƒå¤±è´¥ {path}: {e}")
        
        return avatars
    
    def get_avatar(self, speaker: str, mood: str = "gentle") -> Optional[Image.Image]:
        """è·å–æŒ‡å®šè¯´è¯äººå’Œæƒ…ç»ªçš„å¤´åƒ"""
        if self.enable_mood:
            # ä¼˜å…ˆè¿”å›æƒ…ç»ªç«‹ç»˜
            key = f"{speaker}_{mood}"
            if key in self.avatars:
                return self.avatars[key]
        
        # è¿”å›é»˜è®¤ç«‹ç»˜
        return self.avatars.get(speaker)
    
    def _get_default_font(self) -> str:
        """è·å–ç³»ç»Ÿé»˜è®¤ä¸­æ–‡å­—ä½“"""
        possible_fonts = [
            "/System/Library/Fonts/PingFang.ttc",  # macOS è‹¹æ–¹
            "/System/Library/Fonts/STHeiti Light.ttc",  # macOS é»‘ä½“
            "/usr/share/fonts/truetype/wqy/wqy-zenhei.ttc",  # Linux æ–‡æ³‰é©¿
            "/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttc",  # Linux Noto
            "C:/Windows/Fonts/simhei.ttf",  # Windows é»‘ä½“
            "C:/Windows/Fonts/simsun.ttc",  # Windows å®‹ä½“
        ]
        for font in possible_fonts:
            if os.path.exists(font):
                return font
        return None
    
    def create_subtitle_image(self, text: str, size: Tuple[int, int], 
                             speaker: str = None, mood: str = "gentle") -> np.ndarray:
        """
        åˆ›å»ºå­—å¹•å›¾ç‰‡
        
        Args:
            text: å­—å¹•æ–‡æœ¬
            size: (width, height)
            speaker: è¯´è¯äººæ ‡ç­¾
            mood: æƒ…ç»ªæ ‡ç­¾
        """
        if self.style == "galgame":
            return self._create_galgame_subtitle(text, size, speaker, mood=mood)
        else:
            return self._create_default_subtitle(text, size, speaker, mood=mood)
    
    def _create_default_subtitle(self, text: str, size: Tuple[int, int], 
                                  speaker: str = None, **kwargs) -> np.ndarray:
        """é»˜è®¤æ ·å¼ï¼šå¤´åƒåœ¨å·¦ä¸Šæ–¹ï¼Œæ·±è‰²å­—å¹•æ¡†"""
        width, height = size
        
        # åˆ›å»ºé€æ˜èƒŒæ™¯
        img = Image.new('RGBA', size, (0, 0, 0, 0))
        draw = ImageDraw.Draw(img)
        
        # ========== å…ˆè®¡ç®—æ–‡æœ¬éœ€è¦çš„è¡Œæ•° ==========
        bg_left = 50
        bg_right = width - 50
        avatar_area_width = self.avatar_size + 40
        text_left = bg_left + avatar_area_width
        text_right = bg_right - 30
        max_text_width = text_right - text_left
        
        font, lines = self._get_adaptive_font_and_lines(
            text, max_text_width, float('inf'), self.max_lines
        )
        
        actual_lines = max(len(lines), self.min_lines)
        subtitle_bg_height = self.bg_padding + actual_lines * self.line_height
        min_bg_height = self.avatar_size + 60
        subtitle_bg_height = max(subtitle_bg_height, min_bg_height)
        
        # ========== ç»˜åˆ¶åº•éƒ¨å­—å¹•æ¡† ==========
        bg_bottom = height - 50
        bg_top = bg_bottom - subtitle_bg_height
        
        draw.rectangle(
            [(bg_left, bg_top), (bg_right, bg_bottom)],
            fill=(0, 0, 0, 180)
        )
        
        # ========== ç»˜åˆ¶å¤´åƒå’Œåå­— ==========
        if speaker:
            speaker_name = "Alex" if speaker == "male" else "Cherry"
            label_color = (100, 180, 255) if speaker == "male" else (255, 150, 200)
            
            avatar_x = bg_left + 15
            avatar_y = bg_top + 15
            
            # è·å–å¤´åƒï¼ˆæ”¯æŒæƒ…ç»ªç«‹ç»˜ï¼‰
            avatar = self.get_avatar(speaker, kwargs.get('mood', 'gentle'))
            if avatar:
                img.paste(avatar, (avatar_x, avatar_y), avatar)
                
                try:
                    name_font = ImageFont.truetype(self.font_path, 24) if self.font_path else ImageFont.load_default()
                except:
                    name_font = ImageFont.load_default()
                name_bbox = draw.textbbox((0, 0), speaker_name, font=name_font)
                name_width = name_bbox[2] - name_bbox[0]
                name_x = avatar_x + (self.avatar_size - name_width) // 2
                name_y = avatar_y + self.avatar_size + 8
                draw.text((name_x, name_y), speaker_name, font=name_font, fill=label_color)
        
        # ========== ç»˜åˆ¶å­—å¹•æ–‡æœ¬ ==========
        actual_text_height = len(lines) * self.line_height
        text_start_y = bg_top + (subtitle_bg_height - actual_text_height) // 2
        
        y = text_start_y
        for line in lines:
            bbox = draw.textbbox((0, 0), line, font=font)
            text_width = bbox[2] - bbox[0]
            x = text_left + (max_text_width - text_width) // 2
            draw.text((x, y), line, font=font, fill=(255, 255, 255))
            y += self.line_height
        
        return np.array(img)
    
    def _create_galgame_subtitle(self, text: str, size: Tuple[int, int], 
                                  speaker: str = None, **kwargs) -> np.ndarray:
        """
        GalGame é£æ ¼å­—å¹•ï¼š
        - åŠé€æ˜ç™½è‰²å¯¹è¯æ¡†ï¼ˆåº•éƒ¨å±…ä¸­ï¼Œå…¨å®½ï¼‰
        - åœ†è§’è®¾è®¡
        - åå­—æ ‡ç­¾åœ¨å¯¹è¯æ¡†ä¸Šæ–¹å·¦ä¾§
        - æ¸å˜è¾¹æ¡†
        
        æ³¨æ„ï¼šç«‹ç»˜ä¸åœ¨è¿™é‡Œç»˜åˆ¶ï¼Œè€Œæ˜¯ä½œä¸ºç‹¬ç«‹çš„è§†é¢‘å±‚
        """
        width, height = size
        
        img = Image.new('RGBA', size, (0, 0, 0, 0))
        draw = ImageDraw.Draw(img)
        
        # ========== å¸ƒå±€å‚æ•° ==========
        dialog_margin = 60
        
        # å¯¹è¯æ¡†åŒºåŸŸï¼ˆåº•éƒ¨å±…ä¸­ï¼Œå…¨å®½ï¼‰
        dialog_left = dialog_margin
        dialog_right = width - dialog_margin
        dialog_bottom = height - 40
        dialog_width = dialog_right - dialog_left
        
        # è®¡ç®—æ–‡æœ¬åŒºåŸŸ
        text_padding_left = 50
        text_padding_top = 40
        text_padding_bottom = 30
        max_text_width = dialog_width - text_padding_left * 2
        
        # ========== è®¡ç®—æ–‡æœ¬è¡Œæ•° ==========
        font, lines = self._get_adaptive_font_and_lines(
            text, max_text_width, float('inf'), self.max_lines
        )
        
        actual_lines = max(len(lines), self.min_lines)
        text_height = actual_lines * self.line_height
        
        # å¯¹è¯æ¡†é«˜åº¦
        dialog_height = text_padding_top + text_height + text_padding_bottom
        dialog_top = dialog_bottom - dialog_height
        
        # ========== ç»˜åˆ¶åå­—æ ‡ç­¾ï¼ˆåœ¨å¯¹è¯æ¡†ä¸Šæ–¹å·¦ä¾§ï¼‰ ==========
        if speaker:
            speaker_name = "Alex" if speaker == "male" else "Cherry"
            name_bg_color = (100, 150, 220, 230) if speaker == "male" else (220, 120, 160, 230)
            name_text_color = (255, 255, 255)
            
            try:
                name_font = ImageFont.truetype(self.font_path, 28) if self.font_path else ImageFont.load_default()
            except:
                name_font = ImageFont.load_default()
            
            name_bbox = draw.textbbox((0, 0), speaker_name, font=name_font)
            name_width = name_bbox[2] - name_bbox[0]
            name_height = name_bbox[3] - name_bbox[1]
            
            name_padding_x = 25
            name_padding_y = 8
            name_bg_width = name_width + name_padding_x * 2
            name_bg_height = name_height + name_padding_y * 2
            
            # åå­—æ ‡ç­¾ä½ç½®ï¼ˆå¯¹è¯æ¡†å·¦ä¸Šè§’ä¸Šæ–¹ï¼‰
            name_x = dialog_left + 30
            name_y = dialog_top - name_bg_height + 10  # ç¨å¾®é‡å 
            
            # ç»˜åˆ¶åå­—èƒŒæ™¯ï¼ˆåœ†è§’ï¼‰
            self._draw_rounded_rect(
                draw, 
                (name_x, name_y, name_x + name_bg_width, name_y + name_bg_height),
                radius=8,
                fill=name_bg_color
            )
            
            # ç»˜åˆ¶åå­—
            name_text_x = name_x + name_padding_x
            name_text_y = name_y + name_padding_y - 2
            draw.text((name_text_x, name_text_y), speaker_name, font=name_font, fill=name_text_color)
        
        # ========== ç»˜åˆ¶å¯¹è¯æ¡†ä¸»ä½“ï¼ˆåœ†è§’åŠé€æ˜ï¼‰ ==========
        dialog_bg_color = (255, 255, 255, 200)  # åŠé€æ˜ç™½è‰²
        dialog_border_color = (200, 200, 220, 150)  # æ·¡ç´«è¾¹æ¡†
        
        self._draw_rounded_rect(
            draw,
            (dialog_left, dialog_top, dialog_right, dialog_bottom),
            radius=20,
            fill=dialog_bg_color,
            outline=dialog_border_color,
            width=2
        )
        
        # ç»˜åˆ¶å†…è¾¹æ¡†ï¼ˆè£…é¥°æ•ˆæœï¼‰
        inner_margin = 6
        self._draw_rounded_rect(
            draw,
            (dialog_left + inner_margin, dialog_top + inner_margin, 
             dialog_right - inner_margin, dialog_bottom - inner_margin),
            radius=15,
            outline=(255, 255, 255, 100),
            width=1
        )
        
        # ========== ç»˜åˆ¶å­—å¹•æ–‡æœ¬ï¼ˆå·¦å¯¹é½ï¼‰ ==========
        text_start_x = dialog_left + text_padding_left
        text_start_y = dialog_top + text_padding_top
        
        y = text_start_y
        for line in lines:
            draw.text((text_start_x, y), line, font=font, fill=(50, 50, 60))
            y += self.line_height
        
        # æ³¨æ„ï¼šç«‹ç»˜ä¸å†è¿™é‡Œç»˜åˆ¶ï¼Œè€Œæ˜¯ä½œä¸ºç‹¬ç«‹å±‚åœ¨è§†é¢‘åˆæˆæ—¶æ·»åŠ 
        # è¿™æ ·å¯ä»¥ç¡®ä¿ç«‹ç»˜åœ¨å­—å¹•æ¡†åé¢
        
        return np.array(img)
    
    def _calc_galgame_dialog_top(self, text: str, size: Tuple[int, int]) -> int:
        """
        è®¡ç®— GalGame é£æ ¼å­—å¹•æ¡†çš„é¡¶éƒ¨ Y åæ ‡
        
        Args:
            text: å­—å¹•æ–‡æœ¬
            size: è§†é¢‘å°ºå¯¸ (width, height)
        
        Returns:
            dialog_top: å­—å¹•æ¡†é¡¶éƒ¨ Y åæ ‡
        """
        width, height = size
        
        # ========== å¸ƒå±€å‚æ•°ï¼ˆä¸ _create_galgame_subtitle ä¿æŒä¸€è‡´ï¼‰==========
        dialog_margin = 60
        dialog_left = dialog_margin
        dialog_right = width - dialog_margin
        dialog_bottom = height - 40
        dialog_width = dialog_right - dialog_left
        
        text_padding_left = 50
        text_padding_top = 40
        text_padding_bottom = 30
        max_text_width = dialog_width - text_padding_left * 2
        
        # ========== è®¡ç®—æ–‡æœ¬è¡Œæ•° ==========
        font, lines = self._get_adaptive_font_and_lines(
            text, max_text_width, float('inf'), self.max_lines
        )
        
        actual_lines = max(len(lines), self.min_lines)
        text_height = actual_lines * self.line_height
        
        # å¯¹è¯æ¡†é«˜åº¦
        dialog_height = text_padding_top + text_height + text_padding_bottom
        dialog_top = dialog_bottom - dialog_height
        
        return dialog_top
    
    def get_galgame_avatar_clip(self, size: Tuple[int, int], speaker: str, 
                                mood: str = "gentle", duration: float = 1.0, 
                                fps: int = 30, dialog_top: int = None):
        """
        è·å– GalGame é£æ ¼çš„ç«‹ç»˜è§†é¢‘å±‚ï¼ˆç”¨äºæ”¾åœ¨å­—å¹•åé¢ï¼‰
        
        Args:
            size: è§†é¢‘å°ºå¯¸ (width, height)
            speaker: è¯´è¯äºº
            mood: æƒ…ç»ª
            duration: æŒç»­æ—¶é—´
            fps: å¸§ç‡
            dialog_top: å­—å¹•æ¡†é¡¶éƒ¨ Y åæ ‡ï¼ˆç”¨äºå®šä½ç«‹ç»˜ä½ç½®ï¼‰
        
        Returns:
            ImageClip æˆ– None
        """
        width, height = size
        
        if not speaker:
            return None
        
        # è¯»å–é…ç½®å‚æ•°
        config = self.galgame_avatar_config
        height_ratio = config.get('height_ratio', 0.45)  # é»˜è®¤å å±å¹•é«˜åº¦ 45%
        horizontal_position = config.get('horizontal_position', 0.7)  # é»˜è®¤åœ¨å³ä¾§ 70% ä½ç½®
        vertical_offset = config.get('vertical_offset', -20)  # é»˜è®¤å‘ä¸Šåç§» 20px
        
        # è®¡ç®—ç«‹ç»˜å°ºå¯¸
        avatar_max_height = int(height * height_ratio)
        
        # åŠ è½½ç«‹ç»˜
        avatar_img = self._get_large_avatar_by_height(speaker, avatar_max_height, mood)
        
        if not avatar_img:
            print(f"âš ï¸ æ— æ³•åŠ è½½ç«‹ç»˜: {speaker}-{mood}")
            return None
        
        # ç­‰æ¯”ç¼©æ”¾ç¡®ä¿ä¸è¶…è¿‡æœ€å¤§é«˜åº¦
        if avatar_img.height > avatar_max_height:
            ratio = avatar_max_height / avatar_img.height
            new_width = int(avatar_img.width * ratio)
            avatar_img = avatar_img.resize((new_width, avatar_max_height), Image.Resampling.LANCZOS)
        
        # åˆ›å»ºé€æ˜èƒŒæ™¯çš„å›¾ç‰‡ï¼ˆå…¨å±å°ºå¯¸ï¼‰
        full_img = Image.new('RGBA', size, (0, 0, 0, 0))
        
        # è®¡ç®—ç«‹ç»˜æ°´å¹³ä½ç½®ï¼šæ ¹æ® horizontal_position å‚æ•°
        # horizontal_position 0.0 = æœ€å·¦, 0.5 = å±…ä¸­, 1.0 = æœ€å³
        # ç«‹ç»˜ä¸­å¿ƒç‚¹ä½äº horizontal_position å¯¹åº”çš„ä½ç½®
        avatar_center_x = int(width * horizontal_position)
        avatar_x = avatar_center_x - avatar_img.width // 2
        
        # è®¡ç®—ç«‹ç»˜å‚ç›´ä½ç½®ï¼šè´´ç€å­—å¹•æ¡†ä¸Šæ–¹
        if dialog_top is not None:
            # è´´ç€å­—å¹•æ¡†ä¸Šæ–¹ï¼ŒåŠ ä¸Šå‚ç›´åç§»
            avatar_y = dialog_top - avatar_img.height + vertical_offset
        else:
            # å›é€€ï¼šä½¿ç”¨é»˜è®¤ä½ç½®ï¼ˆå±å¹•åº•éƒ¨åä¸Šï¼‰
            avatar_y = height - avatar_img.height - 100
        
        # ç¡®ä¿ç«‹ç»˜ä¸ä¼šå®Œå…¨è¶…å‡ºå±å¹•
        if avatar_x < -avatar_img.width // 2:
            avatar_x = -avatar_img.width // 2
        if avatar_x > width - avatar_img.width // 2:
            avatar_x = width - avatar_img.width // 2
        
        # å°†ç«‹ç»˜ç²˜è´´åˆ°é€æ˜èƒŒæ™¯ä¸Š
        full_img.paste(avatar_img, (avatar_x, avatar_y), avatar_img)
        
        # è½¬æ¢ä¸º numpy æ•°ç»„å¹¶åˆ›å»º ImageClip
        import numpy as np
        from moviepy import ImageClip as MCImageClip
        
        avatar_clip = MCImageClip(np.array(full_img)).with_duration(duration).with_fps(fps)
        
        return avatar_clip
    
    def _draw_rounded_rect(self, draw, bbox, radius, fill=None, outline=None, width=1):
        """ç»˜åˆ¶åœ†è§’çŸ©å½¢"""
        x1, y1, x2, y2 = bbox
        
        # ä¸»ä½“çŸ©å½¢
        if fill:
            draw.rectangle([x1 + radius, y1, x2 - radius, y2], fill=fill)
            draw.rectangle([x1, y1 + radius, x2, y2 - radius], fill=fill)
            # å››ä¸ªåœ†è§’
            draw.pieslice([x1, y1, x1 + radius * 2, y1 + radius * 2], 180, 270, fill=fill)
            draw.pieslice([x2 - radius * 2, y1, x2, y1 + radius * 2], 270, 360, fill=fill)
            draw.pieslice([x1, y2 - radius * 2, x1 + radius * 2, y2], 90, 180, fill=fill)
            draw.pieslice([x2 - radius * 2, y2 - radius * 2, x2, y2], 0, 90, fill=fill)
        
        # è¾¹æ¡†
        if outline:
            draw.arc([x1, y1, x1 + radius * 2, y1 + radius * 2], 180, 270, fill=outline, width=width)
            draw.arc([x2 - radius * 2, y1, x2, y1 + radius * 2], 270, 360, fill=outline, width=width)
            draw.arc([x1, y2 - radius * 2, x1 + radius * 2, y2], 90, 180, fill=outline, width=width)
            draw.arc([x2 - radius * 2, y2 - radius * 2, x2, y2], 0, 90, fill=outline, width=width)
            draw.line([x1 + radius, y1, x2 - radius, y1], fill=outline, width=width)
            draw.line([x1 + radius, y2, x2 - radius, y2], fill=outline, width=width)
            draw.line([x1, y1 + radius, x1, y2 - radius], fill=outline, width=width)
            draw.line([x2, y1 + radius, x2, y2 - radius], fill=outline, width=width)
    
    def _get_large_avatar(self, speaker: str, target_width: int, mood: str = "gentle") -> Image.Image:
        """è·å–å¤§å°ºå¯¸å¤´åƒï¼ˆç«‹ç»˜é£æ ¼ï¼Œæ”¯æŒæƒ…ç»ªï¼‰"""
        # ç¡®å®šè¦åŠ è½½çš„æ–‡ä»¶è·¯å¾„
        if self.enable_mood:
            # ä¼˜å…ˆå°è¯•æƒ…ç»ªç«‹ç»˜
            mood_path = f"{self.avatar_base_path}/{speaker}-{mood}.png"
            if os.path.exists(mood_path):
                try:
                    img = Image.open(mood_path).convert('RGBA')
                    ratio = target_width / img.width
                    new_height = int(img.height * ratio)
                    img = img.resize((target_width, new_height), Image.Resampling.LANCZOS)
                    return img
                except:
                    pass
        
        # ä½¿ç”¨é»˜è®¤ç«‹ç»˜
        default_path = f"{self.avatar_base_path}/{speaker}.png"
        if os.path.exists(default_path):
            try:
                img = Image.open(default_path).convert('RGBA')
                ratio = target_width / img.width
                new_height = int(img.height * ratio)
                img = img.resize((target_width, new_height), Image.Resampling.LANCZOS)
                return img
            except:
                pass
        
        # å¤±è´¥åˆ™è¿”å›å·²åŠ è½½çš„å¤´åƒ
        return self.get_avatar(speaker, mood)
    
    def _get_large_avatar_by_height(self, speaker: str, target_height: int, mood: str = "gentle") -> Image.Image:
        """è·å–å¤§å°ºå¯¸å¤´åƒï¼ˆæŒ‰ç›®æ ‡é«˜åº¦ç¼©æ”¾ï¼Œç«‹ç»˜é£æ ¼ï¼Œæ”¯æŒæƒ…ç»ªï¼‰"""
        # æƒ…ç»ªåç§°æ˜ å°„ï¼ˆä»£ç ä¸­çš„æƒ…ç»ª -> æ–‡ä»¶åä¸­çš„æƒ…ç»ªï¼‰
        mood_mapping = {
            'gentle': 'neutral',
            'shocked': 'surprised',
            'resigned': 'sad',
            'expectant': 'expectant',
            'confused': 'confused',
            'angry': 'angry',
            'happy': 'happy',
            'confident': 'confident',
            'sad': 'sad'
        }
        
        # ç¡®å®šè¦åŠ è½½çš„æ–‡ä»¶è·¯å¾„
        if self.enable_mood:
            # ä¼˜å…ˆå°è¯•æƒ…ç»ªç«‹ç»˜ï¼ˆä½¿ç”¨æ˜ å°„åçš„åç§°ï¼‰
            mapped_mood = mood_mapping.get(mood, mood)
            mood_path = f"{self.avatar_base_path}/{speaker}-{mapped_mood}.png"
            if os.path.exists(mood_path):
                try:
                    img = Image.open(mood_path).convert('RGBA')
                    ratio = target_height / img.height
                    new_width = int(img.width * ratio)
                    img = img.resize((new_width, target_height), Image.Resampling.LANCZOS)
                    return img
                except Exception as e:
                    print(f"âš ï¸ åŠ è½½ç«‹ç»˜å¤±è´¥ {mood_path}: {e}")
        
        # å›é€€åˆ° neutral ç«‹ç»˜
        neutral_path = f"{self.avatar_base_path}/{speaker}-neutral.png"
        if os.path.exists(neutral_path):
            try:
                img = Image.open(neutral_path).convert('RGBA')
                ratio = target_height / img.height
                new_width = int(img.width * ratio)
                img = img.resize((new_width, target_height), Image.Resampling.LANCZOS)
                return img
            except Exception as e:
                print(f"âš ï¸ åŠ è½½é»˜è®¤ç«‹ç»˜å¤±è´¥ {neutral_path}: {e}")
        
        # å¤±è´¥åˆ™å°è¯•ä½¿ç”¨ _get_large_avatar å¹¶æŒ‰æ¯”ä¾‹è°ƒæ•´
        fallback = self.get_avatar(speaker, mood)
        if fallback:
            try:
                ratio = target_height / fallback.height
                new_width = int(fallback.width * ratio)
                return fallback.resize((new_width, target_height), Image.Resampling.LANCZOS)
            except:
                pass
        return fallback
    
    def _get_adaptive_font_and_lines(self, text: str, max_width: int, 
                                      max_height: int, max_lines: int) -> Tuple[ImageFont.FreeTypeFont, List[str]]:
        """
        è·å–è‡ªé€‚åº”å­—ä½“å¤§å°å’Œæ¢è¡Œåçš„æ–‡æœ¬
        
        ç­–ç•¥:
        1. ä¼˜å…ˆä½¿ç”¨è¾ƒå¤§å­—ä½“
        2. æ ¹æ®æ–‡æœ¬é•¿åº¦è‡ªé€‚åº”è¡Œæ•°ï¼ˆæœ€å°‘2è¡Œï¼Œæœ€å¤šmax_linesè¡Œï¼‰
        3. å¦‚æœæ–‡æœ¬å¾ˆé•¿ï¼Œé€æ¸å‡å°å­—ä½“ä»¥é€‚åº”è¡Œæ•°é™åˆ¶
        
        Returns:
            (å­—ä½“, è¡Œåˆ—è¡¨)
        """
        # å°è¯•çš„å­—ä½“å¤§å°èŒƒå›´ï¼ˆä»å¤§åˆ°å°ï¼‰
        font_sizes = [42, 38, 34, 30, 26, 22, 18]
        
        for font_size in font_sizes:
            try:
                if self.font_path:
                    font = ImageFont.truetype(self.font_path, font_size)
                else:
                    font = ImageFont.load_default()
            except:
                font = ImageFont.load_default()
            
            # æ¢è¡Œï¼ˆä¼ å…¥æœ€å¤§è¡Œæ•°é™åˆ¶ï¼‰
            lines = self._wrap_text_to_lines(text, font, max_width, max_lines)
            
            # æ£€æŸ¥é«˜åº¦æ˜¯å¦åˆé€‚ï¼ˆä½¿ç”¨å›ºå®šçš„è¡Œé«˜ï¼‰
            total_height = len(lines) * self.line_height
            
            # å¦‚æœé«˜åº¦åˆé€‚ä¸”è¡Œæ•°åœ¨é™åˆ¶å†…ï¼Œä½¿ç”¨è¿™ä¸ªå­—ä½“å¤§å°
            if total_height <= max_height and len(lines) <= max_lines:
                return font, lines
            
            # å¦‚æœè¡Œæ•°å¤ªå¤šï¼Œç»§ç»­å°è¯•æ›´å°çš„å­—ä½“
            if len(lines) > max_lines:
                continue
        
        # å¦‚æœæ‰€æœ‰å­—ä½“å¤§å°éƒ½å°è¯•äº†è¿˜æ˜¯ä¸è¡Œï¼Œä½¿ç”¨æœ€å°å­—ä½“å¹¶å¼ºåˆ¶æˆªæ–­
        try:
            font = ImageFont.truetype(self.font_path, 18) if self.font_path else ImageFont.load_default()
        except:
            font = ImageFont.load_default()
        
        lines = self._wrap_text_to_lines(text, font, max_width, max_lines)
        
        # å¦‚æœè¿˜æ˜¯è¶…è¿‡æœ€å¤§è¡Œæ•°ï¼Œæˆªæ–­å¹¶æ·»åŠ çœç•¥å·
        if len(lines) > max_lines:
            lines = lines[:max_lines]
            if lines[-1]:
                lines[-1] = lines[-1][:20] + "..."
        
        return font, lines
    
    def _wrap_text_to_lines(self, text: str, font, max_width: int, max_lines: int) -> List[str]:
        """
        å°†æ–‡æœ¬æ¢è¡Œä¸ºæŒ‡å®šè¡Œæ•°ï¼Œæ™ºèƒ½å¤„ç†æ ‡ç‚¹ç¬¦å·
        """
        # å…ˆå°è¯•å®Œæ•´æ¢è¡Œ
        all_lines = []
        current_line = ""
        
        for char in text:
            test_line = current_line + char
            bbox = font.getbbox(test_line) if hasattr(font, 'getbbox') else (0, 0, len(test_line) * self.font_size * 0.6, self.font_size)
            text_width = bbox[2] - bbox[0] if len(bbox) >= 4 else len(test_line) * self.font_size * 0.6
            
            if text_width > max_width and current_line:
                all_lines.append(current_line)
                current_line = char
            else:
                current_line = test_line
        
        if current_line:
            all_lines.append(current_line)
        
        # å¦‚æœè¡Œæ•°åœ¨é™åˆ¶å†…ï¼Œç›´æ¥è¿”å›
        if len(all_lines) <= max_lines:
            return all_lines
        
        # å¦‚æœè¡Œæ•°è¿‡å¤šï¼Œéœ€è¦åˆå¹¶ä¸€äº›è¡Œï¼ˆå°½é‡ä¿æŒè¯­ä¹‰ï¼‰
        # é‡æ–°è®¡ç®—ï¼Œä½¿ç”¨æ›´çŸ­çš„è¡Œ
        lines = []
        current_line = ""
        avg_chars_per_line = len(text) // max_lines + 1
        
        for i, char in enumerate(text):
            current_line += char
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¢è¡Œ
            should_break = False
            
            # 1. è¾¾åˆ°å¹³å‡å­—ç¬¦æ•°ä¸”å½“å‰å­—ç¬¦æ˜¯æ ‡ç‚¹ç¬¦å·
            if len(current_line) >= avg_chars_per_line and char in 'ï¼Œã€‚ï¼ï¼Ÿã€ï¼›ï¼š':
                should_break = True
            
            # 2. æ£€æŸ¥å®½åº¦æ˜¯å¦è¶…é™
            bbox = font.getbbox(current_line) if hasattr(font, 'getbbox') else (0, 0, len(current_line) * self.font_size * 0.6, self.font_size)
            text_width = bbox[2] - bbox[0] if len(bbox) >= 4 else len(current_line) * self.font_size * 0.6
            
            if text_width > max_width * 0.95:
                should_break = True
            
            # 3. å·²åˆ°æœ€å
            if i == len(text) - 1:
                should_break = True
            
            if should_break and current_line:
                lines.append(current_line)
                current_line = ""
                
                if len(lines) >= max_lines:
                    # å¦‚æœè¾¾åˆ°æœ€å¤§è¡Œæ•°ï¼Œå°†å‰©ä½™å†…å®¹é™„åŠ åˆ°æœ€åä¸€è¡Œ
                    remaining = text[i+1:]
                    if remaining:
                        lines[-1] += remaining[:20]  # åªåŠ ä¸€éƒ¨åˆ†ï¼Œé¿å…æº¢å‡º
                        if len(remaining) > 20:
                            lines[-1] += "..."
                    break
        
        if current_line and len(lines) < max_lines:
            lines.append(current_line)
        
        return lines[:max_lines]
    
    def _wrap_text(self, text: str, font, max_width: int) -> List[str]:
        """è‡ªåŠ¨æ¢è¡Œ - ç§»é™¤æœ«å°¾ç©ºè¡Œ"""
        lines = []
        current_line = ""
        
        # é¢„å¤„ç†ï¼šç§»é™¤æ–‡æœ¬æœ«å°¾çš„ç©ºç™½å­—ç¬¦
        text = text.rstrip()
        
        for char in text:
            test_line = current_line + char
            bbox = font.getbbox(test_line) if hasattr(font, 'getbbox') else (0, 0, len(test_line) * self.font_size * 0.6, self.font_size)
            text_width = bbox[2] - bbox[0] if len(bbox) >= 4 else len(test_line) * self.font_size * 0.6
            
            if text_width > max_width and current_line:
                lines.append(current_line)
                current_line = char
            else:
                current_line = test_line
        
        # æ·»åŠ æœ€åä¸€è¡Œï¼ˆç¡®ä¿ä¸ä¸ºç©ºï¼‰
        if current_line and current_line.strip():
            lines.append(current_line)
        
        return lines if lines else [text]


class PodcastVideoGenerator:
    """æ’­å®¢è§†é¢‘ç”Ÿæˆå™¨"""
    
    def __init__(self, config: Dict = None):
        self.config = config or {}
        self.width = self.config.get('width', 1920)
        self.height = self.config.get('height', 1080)
        self.fps = self.config.get('fps', 30)
        self.enable_mood = self.config.get('enable_mood', True)
        if self.enable_mood:
            print("âœ¨ æƒ…ç»ªç«‹ç»˜åŠŸèƒ½å·²å¯ç”¨")
        self.subtitle_gen = SubtitleGenerator(
            font_path=self.config.get('font_path'),
            font_size=self.config.get('font_size', 40),
            style=self.config.get('subtitle_style', 'default'),
            enable_mood=self.enable_mood,
            avatar_base_path=self.config.get('avatar_base_path', 'avatar'),
            galgame_avatar_config=self.config.get('galgame_avatar', {})
        )
        
    def create_podcast_video(self, 
                           segments: List[DialogueSegment],
                           output_path: str,
                           background_type: str = "gradient",
                           background_path: str = None,
                           add_waveform: bool = False,
                           transition_duration: float = 0.5) -> str:
        """
        åˆ›å»ºæ’­å®¢è§†é¢‘
        
        Args:
            segments: å¯¹è¯æ®µè½åˆ—è¡¨
            output_path: è¾“å‡ºè§†é¢‘è·¯å¾„
            background_type: èƒŒæ™¯ç±»å‹ (gradient, image, video)
            background_path: èƒŒæ™¯å›¾ç‰‡/è§†é¢‘è·¯å¾„
            add_waveform: æ˜¯å¦æ·»åŠ éŸ³é¢‘æ³¢å½¢
            transition_duration: æ®µè½é—´è¿‡æ¸¡æ—¶é—´ï¼ˆç§’ï¼‰
        
        Returns:
            è¾“å‡ºè§†é¢‘è·¯å¾„
        """
        print(f"ğŸ¬ å¼€å§‹ç”Ÿæˆè§†é¢‘æ’­å®¢...")
        print(f"   åˆ†è¾¨ç‡: {self.width}x{self.height}")
        print(f"   æ®µè½æ•°: {len(segments)}")
        print(f"   èƒŒæ™¯ç±»å‹: {background_type}")
        if background_path:
            print(f"   èƒŒæ™¯è·¯å¾„: {background_path}")
        print(f"   æ®µè½é—´éš”: {transition_duration}ç§’")
        
        video_clips = []
        audio_clips = []
        
        for i, seg in enumerate(segments):
            print(f"[{i+1}/{len(segments)}] å¤„ç†: {seg.speaker} - {seg.text[:30]}...")
            
            # åŠ è½½éŸ³é¢‘
            audio_clip = AudioFileClip(seg.audio_path)
            duration = audio_clip.duration
            
            # åˆ›å»ºè§†é¢‘å¸§
            if background_type == "gradient":
                video_clip = self._create_gradient_background(duration, seg.speaker)
            elif background_type == "image" and background_path:
                video_clip = self._create_image_background(duration, background_path)
            else:
                video_clip = self._create_color_background(duration, seg.speaker)
            
            # æ£€æŸ¥å­—å¹•é•¿åº¦ï¼Œè‡ªåŠ¨æ‹†åˆ†é•¿å­—å¹•ï¼ˆè¿”å› [(æ–‡æœ¬, æ—¶é—´æ¯”ä¾‹), ...]ï¼‰
            subtitle_parts_with_ratio = self.subtitle_gen.split_long_text(seg.text, self.width)
            
            # è·å–æƒ…ç»ªæ ‡ç­¾
            mood = getattr(seg, 'mood', 'gentle') if self.enable_mood else 'gentle'
            
            # åˆ¤æ–­æ˜¯å¦ä½¿ç”¨ galgame é£æ ¼ï¼ˆéœ€è¦å•ç‹¬æ·»åŠ ç«‹ç»˜å±‚ï¼‰
            is_galgame_style = self.config.get('subtitle_style', 'default') == 'galgame'
            
            if len(subtitle_parts_with_ratio) > 1:
                # é•¿å­—å¹•æ‹†åˆ†æˆå¤šä¸ªå­ç‰‡æ®µï¼Œæ ¹æ®å†…å®¹æƒé‡åˆ†é…æ—¶é—´
                subtitle_clips = []
                avatar_clips = []  # æ¯ä¸ªå­ç‰‡æ®µçš„ç«‹ç»˜å±‚
                current_start = 0.0
                
                for part_text, time_ratio in subtitle_parts_with_ratio:
                    # æ ¹æ®æƒé‡è®¡ç®—è¯¥æ®µå­—å¹•çš„æ˜¾ç¤ºæ—¶é•¿
                    part_duration = duration * time_ratio
                    
                    # åˆ›å»ºå­—å¹• clip
                    subtitle_img = self.subtitle_gen.create_subtitle_image(
                        part_text, (self.width, self.height), seg.speaker, mood
                    )
                    subtitle_clip = (ImageClip(subtitle_img)
                                   .with_start(current_start)
                                   .with_duration(part_duration)
                                   .with_fps(self.fps))
                    subtitle_clips.append(subtitle_clip)
                    
                    # å¯¹äº galgame é£æ ¼ï¼Œä¸ºæ¯ä¸ªå­ç‰‡æ®µå•ç‹¬åˆ›å»ºç«‹ç»˜å±‚
                    if is_galgame_style and seg.speaker:
                        dialog_top = self.subtitle_gen._calc_galgame_dialog_top(part_text, (self.width, self.height))
                        avatar_clip = self.subtitle_gen.get_galgame_avatar_clip(
                            (self.width, self.height), seg.speaker, mood, 
                            part_duration, self.fps, dialog_top
                        )
                        if avatar_clip:
                            avatar_clip = avatar_clip.with_start(current_start)
                            avatar_clips.append(avatar_clip)
                    
                    current_start += part_duration
                
                # åˆæˆè§†é¢‘ç‰‡æ®µï¼ˆèƒŒæ™¯ + ç«‹ç»˜ + å­—å¹•ï¼‰
                # å±‚çº§ï¼šèƒŒæ™¯åœ¨æœ€åº•å±‚ï¼Œç„¶åæ˜¯ç«‹ç»˜ï¼Œå­—å¹•åœ¨æœ€ä¸Šå±‚
                all_clips = [video_clip]
                all_clips.extend(avatar_clips)
                all_clips.extend(subtitle_clips)
                composite = CompositeVideoClip(all_clips)
            else:
                # æ™®é€šå­—å¹•ï¼ˆåªæœ‰ä¸€æ®µï¼‰
                part_text, _ = subtitle_parts_with_ratio[0]
                subtitle_img = self.subtitle_gen.create_subtitle_image(
                    part_text, (self.width, self.height), seg.speaker, mood
                )
                subtitle_clip = (ImageClip(subtitle_img)
                               .with_duration(duration)
                               .with_fps(self.fps))
                
                # å¯¹äº galgame é£æ ¼ï¼Œåˆ›å»ºç«‹ç»˜å±‚
                if is_galgame_style and seg.speaker:
                    dialog_top = self.subtitle_gen._calc_galgame_dialog_top(part_text, (self.width, self.height))
                    avatar_clip = self.subtitle_gen.get_galgame_avatar_clip(
                        (self.width, self.height), seg.speaker, mood, 
                        duration, self.fps, dialog_top
                    )
                    # åˆæˆè§†é¢‘ç‰‡æ®µï¼ˆèƒŒæ™¯ + ç«‹ç»˜ + å­—å¹•ï¼‰
                    if avatar_clip:
                        composite = CompositeVideoClip([video_clip, avatar_clip, subtitle_clip])
                    else:
                        composite = CompositeVideoClip([video_clip, subtitle_clip])
                else:
                    composite = CompositeVideoClip([video_clip, subtitle_clip])
            
            composite = composite.with_audio(audio_clip)
            
            video_clips.append(composite)
            audio_clips.append(audio_clip)
            
            # æ·»åŠ æ®µè½é—´çš„è¿‡æ¸¡ï¼ˆé™¤äº†æœ€åä¸€æ®µï¼‰
            if transition_duration > 0 and i < len(segments) - 1:
                # åˆ›å»ºè¿‡æ¸¡ç‰‡æ®µï¼ˆé™éŸ³+æ¸éšæ¸ç°æ•ˆæœï¼‰
                if background_type == "gradient":
                    # ä½¿ç”¨ä¸­æ€§æ¸å˜ä½œä¸ºè¿‡æ¸¡
                    trans_bg = self._create_gradient_background(transition_duration, 'male')
                elif background_type == "image" and background_path:
                    trans_bg = self._create_image_background(transition_duration, background_path)
                else:
                    trans_bg = self._create_color_background(transition_duration, 'male')
                
                # åˆ›å»ºé™éŸ³éŸ³é¢‘
                from moviepy.audio.AudioClip import AudioArrayClip
                import numpy as np
                silent_audio = AudioArrayClip(
                    np.zeros((int(transition_duration * 44100), 2)), 
                    fps=44100
                )
                
                trans_clip = trans_bg.with_audio(silent_audio)
                video_clips.append(trans_clip)
        
        # åˆå¹¶æ‰€æœ‰ç‰‡æ®µ
        print("ğŸ”„ åˆå¹¶è§†é¢‘ç‰‡æ®µ...")
        final_video = concatenate_videoclips(video_clips, method="compose")
        
        # æ·»åŠ ç‰‡å¤´ï¼ˆå¯é€‰ï¼‰
        if self.config.get('add_intro', False):
            intro = self._create_intro_clip()
            final_video = concatenate_videoclips([intro, final_video], method="compose")
        
        # è¾“å‡ºè§†é¢‘
        print(f"ğŸ’¾ å¯¼å‡ºè§†é¢‘: {output_path}")
        
        # æ£€æµ‹å¹³å°å¹¶é€‰æ‹©ç¼–ç å™¨
        import platform
        system = platform.system()
        
        # åŸºç¡€å‚æ•°
        write_params = {
            'fps': self.fps,
            'audio_codec': 'aac',
            'temp_audiofile': 'temp-audio.m4a',
            'remove_temp': True,
        }
        
        # macOS ç¼–ç è®¾ç½®
        # æ³¨æ„ï¼šå½“å‰ FFmpeg ç‰ˆæœ¬çš„ VideoToolbox æ€§èƒ½ä¸ä½³ï¼Œä½¿ç”¨è½¯ä»¶ç¼–ç æ›´å¿«
        if system == 'Darwin':
            print("ğŸ’» ä½¿ç”¨è½¯ä»¶ç¼–ç  (libx264 ultrafast)")
            write_params['codec'] = 'libx264'
            write_params['preset'] = 'ultrafast'  # æœ€å¿«é¢„è®¾
            write_params['threads'] = 0  # è‡ªåŠ¨ä½¿ç”¨æ‰€æœ‰ CPU æ ¸å¿ƒ
            write_params['ffmpeg_params'] = ['-b:v', '4000k']
        else:
            # å…¶ä»–å¹³å°ä½¿ç”¨è½¯ä»¶ç¼–ç 
            print("ğŸ’» ä½¿ç”¨è½¯ä»¶ç¼–ç  (libx264)")
            write_params['codec'] = 'libx264'
            write_params['preset'] = 'medium'
            write_params['threads'] = 4
        
        final_video.write_videofile(output_path, **write_params)
        
        # æ¸…ç†
        for clip in video_clips:
            clip.close()
        for clip in audio_clips:
            clip.close()
        final_video.close()
        
        print(f"âœ… è§†é¢‘ç”Ÿæˆå®Œæˆ: {output_path}")
        return output_path
    
    def _create_gradient_background(self, duration: float, speaker: str) -> ImageClip:
        """åˆ›å»ºæ¸å˜èƒŒæ™¯"""
        # æ ¹æ®è¯´è¯äººé€‰æ‹©ä¸åŒçš„æ¸å˜è‰²
        if speaker == 'male':
            colors = [(30, 60, 114), (50, 100, 150)]  # è“è‰²ç³»
        else:
            colors = [(100, 50, 100), (150, 80, 130)]  # ç´«è‰²ç³»
        
        # åˆ›å»ºæ¸å˜å›¾åƒ
        gradient = np.zeros((self.height, self.width, 3), dtype=np.uint8)
        for y in range(self.height):
            ratio = y / self.height
            for c in range(3):
                gradient[y, :, c] = int(colors[0][c] * (1 - ratio) + colors[1][c] * ratio)
        
        return ImageClip(gradient).with_duration(duration).with_fps(self.fps)
    
    def _create_color_background(self, duration: float, speaker: str) -> ImageClip:
        """åˆ›å»ºçº¯è‰²èƒŒæ™¯"""
        if speaker == 'male':
            color = (40, 70, 120)  # æ·±è“
        else:
            color = (120, 60, 100)  # æ·±ç´«
        
        return ColorClip(size=(self.width, self.height), color=color)\
                        .with_duration(duration).with_fps(self.fps)
    
    def _create_image_background(self, duration: float, image_path: str) -> ImageClip:
        """åˆ›å»ºå›¾ç‰‡èƒŒæ™¯"""
        img = Image.open(image_path)
        # è°ƒæ•´å¤§å°å¹¶è£å‰ªä»¥é€‚åº”è§†é¢‘å°ºå¯¸
        img_ratio = img.width / img.height
        video_ratio = self.width / self.height
        
        if img_ratio > video_ratio:
            new_height = self.height
            new_width = int(self.height * img_ratio)
        else:
            new_width = self.width
            new_height = int(self.width / img_ratio)
        
        img = img.resize((new_width, new_height), Image.Resampling.LANCZOS)
        
        # å±…ä¸­è£å‰ª
        left = (new_width - self.width) // 2
        top = (new_height - self.height) // 2
        img = img.crop((left, top, left + self.width, top + self.height))
        
        return ImageClip(np.array(img)).with_duration(duration).with_fps(self.fps)
    
    def _get_system_font(self) -> str:
        """è·å–ç³»ç»Ÿæ”¯æŒçš„ä¸­æ–‡å­—ä½“"""
        font_paths = [
            "/System/Library/Fonts/PingFang.ttc",  # macOS è‹¹æ–¹
            "/System/Library/Fonts/STHeiti Light.ttc",  # macOS é»‘ä½“
            "/System/Library/Fonts/Helvetica.ttc",  # macOS Helvetica
            "/usr/share/fonts/truetype/wqy/wqy-zenhei.ttc",  # Linux æ–‡æ³‰é©¿
            "C:/Windows/Fonts/simhei.ttf",  # Windows é»‘ä½“
            "C:/Windows/Fonts/simsun.ttc",  # Windows å®‹ä½“
            "Arial",  # é»˜è®¤å›é€€
        ]
        for font in font_paths:
            if font == "Arial" or os.path.exists(font):
                return font
        return "Arial"
    
    def _wrap_text_for_title(self, text: str, font_path: str, max_width: int, 
                              initial_font_size: int) -> Tuple[str, int]:
        """
        æ ¹æ®å®½åº¦è‡ªåŠ¨è°ƒæ•´å­—ä½“å¤§å°å¹¶æ¢è¡Œ
        
        Returns:
            (æ¢è¡Œåçš„æ–‡æœ¬, å®é™…ä½¿ç”¨çš„å­—ä½“å¤§å°)
        """
        from PIL import ImageFont
        
        # å°è¯•ä¸åŒçš„å­—ä½“å¤§å°
        for font_size in range(initial_font_size, 20, -5):  # ä»å¤§åˆ°å°å°è¯•
            try:
                font = ImageFont.truetype(font_path, font_size)
            except:
                font = ImageFont.load_default()
            
            # å°è¯•ä¸æ¢è¡Œ
            try:
                bbox = font.getbbox(text)
                text_width = bbox[2] - bbox[0]
            except:
                text_width = len(text) * font_size * 0.6
            
            if text_width <= max_width:
                # å­—ä½“å¤§å°åˆé€‚ï¼Œä¸éœ€è¦æ¢è¡Œ
                return text, font_size
            
            # éœ€è¦æ¢è¡Œï¼Œå°è¯•æ‰¾åˆ°æœ€ä½³æ¢è¡Œä½ç½®
            lines = []
            current_line = ""
            
            for char in text:
                test_line = current_line + char
                try:
                    bbox = font.getbbox(test_line)
                    line_width = bbox[2] - bbox[0]
                except:
                    line_width = len(test_line) * font_size * 0.6
                
                if line_width > max_width and current_line:
                    lines.append(current_line)
                    current_line = char
                else:
                    current_line = test_line
            
            if current_line:
                lines.append(current_line)
            
            # æ£€æŸ¥è¡Œæ•°æ˜¯å¦åˆé€‚ï¼ˆæœ€å¤š2è¡Œï¼‰
            if len(lines) <= 2:
                return "\n".join(lines), font_size
        
        # å¦‚æœéƒ½ä¸è¡Œï¼Œè¿”å›æœ€å°å­—ä½“çš„å¼ºåˆ¶æ¢è¡Œ
        try:
            font = ImageFont.truetype(font_path, 20)
        except:
            font = ImageFont.load_default()
        
        lines = []
        current_line = ""
        for char in text:
            test_line = current_line + char
            try:
                bbox = font.getbbox(test_line)
                line_width = bbox[2] - bbox[0]
            except:
                line_width = len(test_line) * 20 * 0.6
            
            if line_width > max_width and current_line:
                lines.append(current_line)
                current_line = char
            else:
                current_line = test_line
        
        if current_line:
            lines.append(current_line)
        
        return "\n".join(lines[:3]), 20  # æœ€å¤š3è¡Œ
    
    def _create_intro_clip(self, title_text: str = None, subtitle_text: str = None) -> CompositeVideoClip:
        """åˆ›å»ºç‰‡å¤´ - æ”¯æŒè‡ªåŠ¨è°ƒæ•´å­—ä½“å¤§å°å’Œæ¢è¡Œ"""
        duration = 3
        
        # èƒŒæ™¯
        bg = self._create_gradient_background(duration, 'male')
        
        # æ ‡é¢˜æ–‡æœ¬ - ä½¿ç”¨ä¼ å…¥çš„å‚æ•°æˆ–é»˜è®¤å€¼
        title_text = title_text or self.config.get('title', 'æ–‡çŒ®è§£è¯»')
        subtitle_text = subtitle_text or self.config.get('subtitle', 'å¯¹è¯å¼ç§‘æ™®æ’­å®¢')
        
        # è·å–ä¸­æ–‡å­—ä½“
        font = self._get_system_font()
        
        # è®¡ç®—å®‰å…¨è¾¹è·
        margin = int(self.width * 0.1)  # å·¦å³å„10%è¾¹è·
        max_text_width = self.width - 2 * margin
        
        # å¤„ç†ä¸»æ ‡é¢˜ - è‡ªåŠ¨è°ƒæ•´å­—ä½“å¤§å°å’Œæ¢è¡Œ
        wrapped_title, title_font_size = self._wrap_text_for_title(
            title_text, font, max_text_width, initial_font_size=80
        )
        
        print(f"   ä¸»æ ‡é¢˜: {title_text[:30]}...")
        print(f"   å­—ä½“å¤§å°: {title_font_size}px")
        if "\n" in wrapped_title:
            print(f"   å·²è‡ªåŠ¨æ¢è¡Œ: {wrapped_title.count(chr(10)) + 1} è¡Œ")
        
        title = TextClip(
            text=wrapped_title,
            font_size=title_font_size,
            color='white',
            font=font,
            stroke_color='black',
            stroke_width=2,
            method='label',
            text_align='center'
        ).with_duration(duration).with_position('center')
        
        # å¤„ç†å‰¯æ ‡é¢˜ - å­—ä½“å°ä¸€äº›
        wrapped_subtitle, subtitle_font_size = self._wrap_text_for_title(
            subtitle_text, font, max_text_width, initial_font_size=40
        )
        
        # æ ¹æ®ä¸»æ ‡é¢˜è¡Œæ•°è°ƒæ•´å‰¯æ ‡é¢˜ä½ç½®
        title_lines = wrapped_title.count('\n') + 1
        subtitle_y = self.height * 0.55 + title_lines * title_font_size * 0.3
        
        subtitle = TextClip(
            text=wrapped_subtitle,
            font_size=subtitle_font_size,
            color='yellow',
            font=font,
            method='label',
            text_align='center'
        ).with_duration(duration).with_position(('center', subtitle_y))
        
        intro = CompositeVideoClip([bg, title, subtitle])
        try:
            # moviepy 2.x
            intro = FadeIn(duration=0.5).apply(intro)
            intro = FadeOut(duration=0.5).apply(intro)
        except:
            # moviepy 1.x
            intro = intro.fx(fadein, duration=0.5)
            intro = intro.fx(fadeout, duration=0.5)
        
        return intro


class AudioVideoPipeline:
    """éŸ³é¢‘åˆ°è§†é¢‘çš„å®Œæ•´æµç¨‹"""
    
    def __init__(self, video_config: Dict = None):
        self.video_config = video_config or {
            'width': 1920,
            'height': 1080,
            'fps': 30,
            'font_size': 40,
            'font_path': None,
            'add_intro': True,
            'title': 'æ–‡çŒ®è§£è¯»',
            'subtitle': 'å¯¹è¯å¼ç§‘æ™®æ’­å®¢'
        }
    
    def run(self, 
           audio_dir: str,
           markdown_path: str,
           output_path: str = "podcast_video.mp4",
           background_type: str = "gradient",
           background_path: str = None,
           title: str = None,
           subtitle: str = None,
           transition_duration: float = 0.5) -> str:
        """
        æ‰§è¡Œå®Œæ•´æµç¨‹
        
        Args:
            audio_dir: éŸ³é¢‘æ–‡ä»¶ç›®å½•
            markdown_path: Markdown æ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºè·å–æ–‡æœ¬ï¼‰
            output_path: è¾“å‡ºè§†é¢‘è·¯å¾„
            background_type: èƒŒæ™¯ç±»å‹
            background_path: èƒŒæ™¯å›¾ç‰‡/è§†é¢‘è·¯å¾„ï¼ˆå½“ background_type=image æ—¶ä½¿ç”¨ï¼‰
            title: ç‰‡å¤´æ ‡é¢˜ï¼ˆå¯é€‰ï¼‰
            subtitle: ç‰‡å¤´å‰¯æ ‡é¢˜ï¼ˆå¯é€‰ï¼‰
            transition_duration: æ®µè½é—´è¿‡æ¸¡æ—¶é—´ï¼ˆç§’ï¼‰
        """
        # 1. è§£æ Markdown è·å–å¯¹è¯æ–‡æœ¬
        print("ğŸ“– è§£æå¯¹è¯æ–‡æœ¬...")
        dialogues = self._parse_markdown(markdown_path)
        
        # 2. åŒ¹é…éŸ³é¢‘æ–‡ä»¶
        print("ğŸµ åŒ¹é…éŸ³é¢‘æ–‡ä»¶...")
        segments = self._match_audio_files(dialogues, audio_dir)
        
        if not segments:
            raise ValueError("æœªæ‰¾åˆ°åŒ¹é…çš„éŸ³é¢‘æ–‡ä»¶")
        
        print(f"   æ‰¾åˆ° {len(segments)} ä¸ªéŸ³é¢‘ç‰‡æ®µ")
        
        # æ›´æ–°è§†é¢‘é…ç½®
        if title:
            self.video_config['title'] = title
        if subtitle:
            self.video_config['subtitle'] = subtitle
        
        # 3. ç”Ÿæˆè§†é¢‘
        generator = PodcastVideoGenerator(self.video_config)
        output = generator.create_podcast_video(
            segments, 
            output_path,
            background_type=background_type,
            background_path=background_path,
            transition_duration=transition_duration
        )
        
        return output
    
    def _parse_markdown(self, markdown_path: str) -> List[Dict]:
        """è§£æ Markdown æ–‡ä»¶ï¼ˆæ”¯æŒæƒ…ç»ªæ ‡æ³¨ï¼‰"""
        with open(markdown_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        dialogues = []
        
        # é¦–å…ˆå°è¯•è§£ææ–°æ ¼å¼ï¼ˆå¸¦æƒ…ç»ªï¼‰
        # æ–°æ ¼å¼: ### speaker ### \n ### mood ### \n ### text ###
        new_pattern = r'###\s*(male|female)\s*speaker\s*###\s*\n\s*###\s*(\w+)\s*###\s*\n\s*###\s*(.*?)\s*###'
        new_matches = re.findall(new_pattern, content, re.DOTALL)
        
        # æ—§æ ¼å¼: ### speaker ### \n ### text ###
        old_pattern = r'###\s*(male|female)\s*speaker\s*###\s*\n\s*###\s*(.*?)\s*###'
        old_matches = re.findall(old_pattern, content, re.DOTALL)
        
        # å¦‚æœæ–°æ ¼å¼åŒ¹é…æˆåŠŸä¸”æ•°é‡åˆç†ï¼Œä½¿ç”¨æ–°æ ¼å¼
        if new_matches and len(new_matches) >= len(old_matches) / 2:
            for idx, (speaker, mood, text) in enumerate(new_matches, 1):
                text = self._clean_text(text)
                if text:
                    dialogues.append({
                        'index': idx,
                        'speaker': speaker.lower(),
                        'text': text,
                        'mood': mood.lower()
                    })
        else:
            # ä½¿ç”¨æ—§æ ¼å¼è§£æï¼Œæƒ…ç»ªé»˜è®¤ä¸º gentle
            for idx, (speaker, text) in enumerate(old_matches, 1):
                text = self._clean_text(text)
                if text:
                    dialogues.append({
                        'index': idx,
                        'speaker': speaker.lower(),
                        'text': text,
                        'mood': 'gentle'
                    })
        
        return dialogues
    
    def _match_audio_files(self, dialogues: List[Dict], audio_dir: str) -> List[DialogueSegment]:
        """åŒ¹é…éŸ³é¢‘æ–‡ä»¶"""
        segments = []
        audio_dir = Path(audio_dir)
        
        for d in dialogues:
            # å¯»æ‰¾åŒ¹é…çš„éŸ³é¢‘æ–‡ä»¶
            pattern = f"*{d['index']:03d}*{d['speaker']}*.wav"
            matching_files = list(audio_dir.glob(pattern))
            
            if matching_files:
                audio_path = str(matching_files[0])
                # è·å–éŸ³é¢‘æ—¶é•¿
                try:
                    audio = AudioFileClip(audio_path)
                    duration = audio.duration
                    audio.close()
                except:
                    duration = 0
                
                segments.append(DialogueSegment(
                    index=d['index'],
                    speaker=d['speaker'],
                    text=d['text'],
                    audio_path=audio_path,
                    duration=duration,
                    mood=d.get('mood', 'gentle')
                ))
        
        return segments
    
    def _clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬"""
        # ç§»é™¤å„ç§æ¢è¡Œç¬¦å’Œå¤šä½™ç©ºç™½
        text = text.replace('\n', ' ').replace('\r', ' ')
        text = re.sub(r'\s+', ' ', text).strip()
        # ç§»é™¤æ‹¬å·å†…çš„å†…å®¹ï¼ˆå¦‚å¤‡æ³¨ï¼‰
        text = re.sub(r'[ï¼ˆ(][^ï¼‰)]+[ï¼‰)]', '', text)
        # ç¡®ä¿æ–‡æœ¬æœ«å°¾æ²¡æœ‰å¤šä½™ç©ºç™½
        text = text.rstrip()
        return text


def load_config(config_path: str = "configs/video/config.yaml") -> Dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    default_config = {
        'audio_dir': 'audio_output',
        'markdown_file': 'paperwork_in/æ–‡çŒ®è§£è¯»å¯¹è¯æ–‡æ¡ˆ-2.md',
        'output_dir': 'broadcast_output',
        'output_filename': '',
        'resolution': {'width': 1920, 'height': 1080},
        'fps': 30,
        'background_type': 'gradient',
        'background_image': '',
        'show_intro': True,
        'title': '',
        'subtitle': 'å¯¹è¯å¼ç§‘æ™®æ’­å®¢',
        'transition_duration': 0.5,
        'male_avatar': 'avatar/male.png',
        'female_avatar': 'avatar/female.png',
        'male_name': 'Alex',
        'female_name': 'Cherry',
        'subtitle_style': 'default',
        'font_size': 40,
        'enable_mood': True,  # æƒ…ç»ªåŠŸèƒ½å¼€å…³ï¼Œé»˜è®¤å¼€å¯
        'avatar_base_path': 'avatar',  # ç«‹ç»˜åŸºç¡€è·¯å¾„
        'galgame_avatar': {  # GalGame é£æ ¼ç«‹ç»˜é…ç½®
            'height_ratio': 0.45,  # ç«‹ç»˜é«˜åº¦å å±å¹•æ¯”ä¾‹ï¼ˆé»˜è®¤ 45%ï¼‰
            'horizontal_position': 0.7,  # æ°´å¹³ä½ç½®ï¼ˆ0.0=å·¦, 0.5=ä¸­, 1.0=å³ï¼‰
            'vertical_offset': -20,  # å‚ç›´åç§»ï¼ˆåƒç´ ï¼Œè´Ÿå€¼å‘ä¸Šï¼‰
        },
    }
    
    if os.path.exists(config_path):
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                user_config = yaml.safe_load(f)
                if user_config:
                    default_config.update(user_config)
                    print(f"âœ… å·²åŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
        except Exception as e:
            print(f"âš ï¸  åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
    else:
        print(f"âš ï¸  æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶ {config_path}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        print(f"   æç¤º: å¤åˆ¶ configs/video/config.yaml.example è¿›è¡Œä¿®æ”¹")
    
    return default_config


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='è§†é¢‘æ’­å®¢ç”Ÿæˆå™¨ - å°†éŸ³é¢‘è½¬æ¢ä¸ºå¸¦å­—å¹•çš„è§†é¢‘',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨æ–¹å¼:
  # æ–¹å¼1: ä½¿ç”¨é…ç½®æ–‡ä»¶ï¼ˆæ¨èï¼‰
  python video_generator.py
  
  # æ–¹å¼2: æŒ‡å®šé…ç½®æ–‡ä»¶è·¯å¾„
  python video_generator.py -c configs/video/my_config.yaml
  
  # æ–¹å¼3: å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®
  python video_generator.py -i audio_output -m æ–‡çŒ®.md

æç¤º:
  â€¢ é¦–æ¬¡ä½¿ç”¨è¯·å¤åˆ¶ configs/video/config.yaml.example ä¸º configs/video/config.yaml
  â€¢ éŸ³é¢‘æ–‡ä»¶å‘½åæ ¼å¼: dialogue_001_male.wav, dialogue_002_female.wav
  â€¢ ç”·å£°æ˜¾ç¤º Alex å¤´åƒ(è“è‰²)ï¼Œå¥³å£°æ˜¾ç¤º Cherry å¤´åƒ(ç²‰è‰²)
  â€¢ æ”¯æŒæƒ…ç»ªç«‹ç»˜: åœ¨å¯¹è¯è„šæœ¬ä¸­æ·»åŠ  mood æ ‡ç­¾ï¼Œå¦‚ ### happy ###
        """
    )
    
    parser.add_argument(
        '-c', '--config',
        default='configs/video/config.yaml',
        help='é…ç½®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: configs/video/config.yaml)'
    )
    parser.add_argument(
        '-i', '--input',
        default=None,
        help='éŸ³é¢‘æ–‡ä»¶ç›®å½• (è¦†ç›–é…ç½®æ–‡ä»¶)'
    )
    parser.add_argument(
        '-m', '--markdown',
        default=None,
        help='Markdown æ–‡ä»¶è·¯å¾„ (è¦†ç›–é…ç½®æ–‡ä»¶)'
    )
    parser.add_argument(
        '-o', '--output',
        default=None,
        help='è¾“å‡ºè§†é¢‘è·¯å¾„ (è¦†ç›–é…ç½®æ–‡ä»¶)'
    )
    parser.add_argument(
        '-t', '--title',
        default=None,
        help='ç‰‡å¤´æ ‡é¢˜ (è¦†ç›–é…ç½®æ–‡ä»¶)'
    )
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    config = load_config(args.config)
    
    # å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®æ–‡ä»¶
    if args.input:
        config['audio_dir'] = args.input
    if args.markdown:
        config['markdown_file'] = args.markdown
    if args.output:
        config['output_filename'] = args.output
    if args.title:
        config['title'] = args.title
    
    # æ£€æŸ¥ä¾èµ–
    try:
        import moviepy
    except ImportError:
        print("âŒ è¯·å…ˆå®‰è£…ä¾èµ–: pip install moviepy Pillow numpy")
        return
    
    # æ£€æŸ¥å¿…è¦å‚æ•°
    # å¦‚æœ audio_dir æ˜¯ tts_outputï¼Œå°è¯•æŸ¥æ‰¾æœ€æ–°çš„æ—¶é—´ç¼–å·å­æ–‡ä»¶å¤¹
    audio_dir = config['audio_dir']
    if audio_dir == 'tts_output' and os.path.exists(audio_dir):
        try:
            subdirs = [d for d in os.listdir(audio_dir) 
                      if os.path.isdir(os.path.join(audio_dir, d)) and d[0].isdigit()]
            if subdirs:
                # æŒ‰åç§°æ’åºè·å–æœ€æ–°çš„æ—¶é—´æ–‡ä»¶å¤¹
                subdirs.sort(reverse=True)
                latest_subdir = subdirs[0]
                audio_dir = os.path.join(audio_dir, latest_subdir)
                print(f"ğŸ“ è‡ªåŠ¨ä½¿ç”¨æœ€æ–°çš„ TTS è¾“å‡ºç›®å½•: {audio_dir}")
        except Exception:
            pass
    config['audio_dir'] = audio_dir
    
    if not os.path.exists(config['audio_dir']):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°éŸ³é¢‘ç›®å½• '{config['audio_dir']}'")
        print(f"   è¯·å…ˆè¿è¡Œ: python tts_generator.py {config['markdown_file']}")
        return
    
    if not os.path.exists(config['markdown_file']):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ° Markdown æ–‡ä»¶ '{config['markdown_file']}'")
        return
    
    # ç”Ÿæˆè¾“å‡ºè·¯å¾„
    if config['output_filename']:
        output_path = os.path.join(config['output_dir'], config['output_filename'])
    else:
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        output_path = os.path.join(config['output_dir'], f"podcast_{timestamp}.mp4")
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(config['output_dir'], exist_ok=True)
    
    # å‡†å¤‡è§†é¢‘é…ç½®
    video_config = {
        'width': config['resolution']['width'],
        'height': config['resolution']['height'],
        'fps': config['fps'],
        'add_intro': config['show_intro'],
        'title': config['title'] or os.path.splitext(os.path.basename(config['markdown_file']))[0],
        'subtitle': config['subtitle'],
        'subtitle_style': config.get('subtitle_style', 'default'),
        'font_size': config.get('font_size', 40),
        'enable_mood': config.get('enable_mood', True),
        'avatar_base_path': config.get('avatar_base_path', 'avatar'),
        'galgame_avatar': config.get('galgame_avatar', {}),
    }
    
    # è¿è¡Œæµç¨‹
    print(f"\nğŸ¬ å¼€å§‹ç”Ÿæˆè§†é¢‘...")
    print(f"   éŸ³é¢‘ç›®å½•: {config['audio_dir']}")
    print(f"   è¾“å‡ºæ–‡ä»¶: {output_path}")
    print(f"   èƒŒæ™¯ç±»å‹: {config['background_type']}")
    print(f"   æ®µè½é—´éš”: {config['transition_duration']}ç§’")
    
    pipeline = AudioVideoPipeline(video_config)
    
    try:
        output = pipeline.run(
            audio_dir=config['audio_dir'],
            markdown_path=config['markdown_file'],
            output_path=output_path,
            background_type=config['background_type'],
            background_path=config['background_image'] if config['background_type'] == 'image' else None,
            title=config['title'] if config['show_intro'] else None,
            subtitle=config['subtitle'] if config['show_intro'] else None,
            transition_duration=config['transition_duration']
        )
        print(f"\nâœ… è§†é¢‘æ’­å®¢ç”ŸæˆæˆåŠŸ!")
        print(f"ğŸ“ æ–‡ä»¶ä½ç½®: {output}")
    except Exception as e:
        print(f"\nâŒ ç”Ÿæˆå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()
